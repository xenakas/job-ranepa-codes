{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "259d081a",
   "metadata": {},
   "source": [
    "### Методология сбора и обработки данных с применением машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6869b22",
   "metadata": {},
   "source": [
    "#### Сбор"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b89f15",
   "metadata": {},
   "source": [
    "Those pipelines can be accessed and tested by the user at any\n",
    "given time when the algorithm runs. This frameworks allows the flexibility to assess the outcome from\n",
    "each step, where users can check if the proposed branch of data cleaning pipelines is most promising\n",
    "and should be considered as a candidate solution.\n",
    "3.5 CPClean: Reusable Computation in Data Cleaning\n",
    "In the Nearest Neighbor Classifiers over Incomplete Information paper[6], The authors proposed several\n",
    "solutions to curb the problem of these inconsistencies that are assumed to impact machine learning.\n",
    "Such solutions include using checking and counting as tools for studying the impact of incomplete data\n",
    "on training machine learning applications. The paper also proposes an extensive data collection for\n",
    "3\n",
    "the machine learning approach, the CPClean, which is developed on top of the CP primitives whose\n",
    "performance has more significance on existing work, especially on datasets with systematic values.\n",
    "Such is seen when comparing conventional data collecting approaches with CPClean. When working\n",
    "on 5 datasets, all with systematic missingness, CPClean closes a 100% gap on average by cleaning up\n",
    "to 36% of dirty data. In contrast, the other data collecting approaches, such as BoostClean, which\n",
    "is the best automatic cleaning approach, can only clean a 14% gap on average. CPClean has an\n",
    "advantage over other cleaning approaches because it is most appealing on data sets with systematic\n",
    "missingness as it closes 100% in all cases where other techniques such as BoostClean fail to achieve\n",
    "good performance[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b4628c",
   "metadata": {},
   "source": [
    "where the parameter α is a k-vector with components αi > 0, and where Γ(x) is the Gamma\n",
    "function\n",
    "2.7 Dirichlet Process (over a Gaussian)\n",
    "constant α going higher as figures go down (1,10,100,1000\n",
    "2.8 Dirichlet Distribution - Observations\n",
    "• ’distribution over distributions’\n",
    "• Natural choice of prior for discrete variable\n",
    "• Is a probability distribution on discrete probability distributions - although it’s a continuous\n",
    "distribution\n",
    "• depending on choice of α distribution can favor sparse discrete probabilities over k - high α\n",
    "gives more ’topics per document and vice versa\n",
    "• for k = 2 Dirichlet converges to Beta distribution\n",
    "•\n",
    "E(θi) = αi\n",
    "∑ αi\n",
    "3 Inference\n",
    "3.1 Framing the Problem\n",
    "1. Known - W - The ’words’\n",
    "2. Unknown - β - distribution over works per topic ( K × |W | matrix\n",
    "3. Latent variable - Z - topic per word\n",
    "4. Latent variable - θ - distribution of topics per document\n",
    "4\n",
    "The two latent variable scale with the size of the input data where β is a fixed size input.\n",
    "Inference of latent variables requires calculation of the posterior:\n",
    "p(θ, z|w, α, β) = p(θ, z, w|α, β)\n",
    "p(w|α, β) (6)\n",
    "However, the denominator is intractable. Two main approaches are typically used for inference:\n",
    "1. Variational Inference - via EM - faster and in the original paper\n",
    "2. Gibbs Sampling via MCMC (or collapsed Gibbs sampling) - exact\n",
    "3.2 Variational Inference\n",
    "1. Simplifying assumption is made on the posterior. In this case the ’mean-field assumption’,\n",
    "where each latent variable is controlled by it’s own variational variable to define a class of\n",
    "possible approximate posteriors q:\n",
    "q(θ, z|γ) = q(θ|γθ)\n",
    "N∏\n",
    "n=1\n",
    "q(zn|γn)\n",
    "2. Derive an algorithm for computing γ in a way that minimize the KL divergence, or equivalently\n",
    "maximizing the ELBO, of q with respect to true posterior p as a function of γ\n",
    "3. Implement EM algorithm until ELBO converges:\n",
    "(a) E-step: compute γ per document as discussed in (2)\n",
    "(b) M-step: with choice of γ maximize the lower bound on log likelihood of p(w|α, β) - This\n",
    "corresponds to maximum likelihood under the approximate posterior from E-step.\n",
    "• ’ELBO’: evidence lower bound\n",
    "log p(w|α, β) = L(γ; α, β) + KL(q, p)\n",
    "L(γ; α, β) = Eq[log P (θ, z, w|α, β)] − Eq[log q(θ, z)]\n",
    "• Solutions to both the E and M steps are iterative and can be derived by taking derivatives\n",
    "of ELBO with respect to the relevant variables - ’coordinate ascent’\n",
    "3.3 MCMC\n",
    "Gibbs Sampling:\n",
    "• In this setting can treat β as a latent variable too.\n",
    "• p(β, θ, z|w) ∼ {GibbsSample}\n",
    "• start with initial guesses for β, θ, z\n",
    "• iterate through all dimensions of each variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54106d05",
   "metadata": {},
   "source": [
    "#### Обработка "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R 4.0.2",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
